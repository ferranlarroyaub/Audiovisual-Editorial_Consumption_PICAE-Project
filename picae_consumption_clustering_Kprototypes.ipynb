{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-rider",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "\n",
    "### 1- Clustering. K-Prototypes Algorithm\n",
    "    1.1- Standardise continous dimensions\n",
    "    1.2- Ideal number of clústers (cost function)\n",
    "    1.3- Clustering. K-Prototypes Algorithm\n",
    "    \n",
    " \n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-segment",
   "metadata": {},
   "source": [
    "# 1- Clustering. K-Prototypes Algorithm\n",
    "\n",
    "\n",
    "In order to group the data into consumption groups, we use a clustering algorithm. We propose K-Prototypes, which is the version of the well-known K-Means and K-Modes algorithm, for mixed data (which combines continuous and discreet data). \n",
    "\n",
    "We first explain how both K-Means (continous data) and K-modes (discrete/categorical data) work:\n",
    "\n",
    "The K-Means algorithm for continuous data works as follows: \n",
    "\n",
    "    1. We choose the number of groups/clusters in which we want to group the data.\n",
    "    \n",
    "    2. The algorithm selects the \"centroids\" of each cluster. There are different methods to initially select centroids, such as doing so randomly. This algorithm uses by default the initialization method called \"Cao\" (Cao, Fuyuan, Jiye Liang, and Liang Bai. \"A new initialization method for categorical data clustering.\" Expert Systems with Applications 36.7 (2009): 10223-10228)\n",
    "    \n",
    "    3. Each data (unit of consumption) is assigned to the nearest cluster. This is done by calculating the Euclidian distance from each unit of consumption to the centroids, and each unit of consumption is assigned to the nearest cluster (shorter distance to the corresponding centroid). Then the position of the centroids of each cluster is re-calculated as the arithmetic mean of all the positions in the cluster.  \n",
    "    \n",
    "    4. So far the K-Means algorithm has done 1 iteration. These steps are repeated until the positions of centroids no longer change, meaning that clusters are well defined and distinguishable.  \n",
    "\n",
    "   \n",
    "For discrete/categorical data, the algorithm used analogous to K-Means is K-Modes. The procedure is exactly the same as the one described above by the continuous version (K-Means).  However, in this case the data is of a discrete type and therefore a Euclidean distance between the data cannot be defined. Then another concept of distance called \"similarity function\" is used. It consists of comparing two consumption units and defining the distance between them as follows:\n",
    "   \n",
    "    1. 0 is added to the distance value for each attribute shared by both consumer units. \n",
    "\n",
    "    2. 1 is added to the distance value for each different dimension or attribute.  \n",
    "    \n",
    "    3. As in K-Means, each unit of consumption is assigned to the nearest cluster (calculating the distance with the \"similarity function\" in each centroide).  \n",
    "    \n",
    "    4. In this case, centroid positions cannot be calculated with the arithmetic mean and are re-calculated by assigning the values of the most frequent attributes in their own clusters.  \n",
    "\n",
    "\n",
    "The K-Prototypes algorithm uses K-Means for those continuous dimensions and K-Modes for discrete/categorical ones. In this way, we have an algorithm capable of grouping consumer units that are of mixed type, without any method of coding discrete variables to transform them into continuous ones. \n",
    "\n",
    "\n",
    "#### Inputs of the algorithm:\n",
    "\n",
    "    1. Number of clusters (we have to choose it)\n",
    "    2. Data to be clusterized, specifying which dimensions are discrete.\n",
    "    \n",
    "#### Outputs:\n",
    "    \n",
    "    1. Cost function: It is defined as sum of the distances (weighted) between all consumption units to their respective centroids of the cluster. \n",
    "    2. Number of iterations needed to converge.\n",
    "    3. Coordinates/positions of all the centroids.\n",
    "    4. Cluster labels for each unit of consumption.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### References:\n",
    "\n",
    "    1. Huang, Z.: Clustering large data sets with mixed numeric and categorical values, Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference, Singapore, pp. 21-34, 1997. \n",
    "\n",
    "    2. https://github.com/nicodv/kmodes \n",
    "\n",
    "    3. K-Prototypes - Customer Clustering with Mixed Data Types | Well Enough (antonsruberts.github.io) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-fabric",
   "metadata": {},
   "source": [
    "## 1.1- Standardise continous dimensions\n",
    "We standardise continuous data so that all have the same scale and without units, we do this by removing the average value and dividing by the standard deviation. In this way we avoid giving more importance to some dimensions compared to others due to the scale or units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compatible-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('consumption_data_logged_processed.csv')   # Read data file\n",
    "\n",
    "df['player_id']=df['player_id'].astype(str)   # Column to string (needed for the algorithm)\n",
    "df['contingut_id']=df['contingut_id'].astype(str)\n",
    "df['programa_capitol']=df['programa_capitol'].astype(str)\n",
    "df['year']=df['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-enough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<min_data>= 50163.67049855757  std min_data= 19061.224481347042\n",
      "<durada_consum>= 1288.9360753883802  std durada_consum= 2980.405043602095\n",
      "<durada>= 2422.884191680986  std durada= 2480.3205253368405\n"
     ]
    }
   ],
   "source": [
    "# standardizing data\n",
    "columns_to_normalize= ['min_data','durada_consum','durada']\n",
    "\n",
    "\n",
    "# mean values, to return to the original values after clustering\n",
    "min_data_mean=np.mean(df['min_data'])\n",
    "min_data_std=np.std(df['min_data'])\n",
    "durada_consum_mean=np.mean(df['durada_consum'])\n",
    "durada_consum_std=np.std(df['durada_consum'])\n",
    "durada_mean=np.mean(df['durada'])\n",
    "durada_std=np.std(df['durada'])\n",
    "print('<min_data>=',min_data_mean,'', 'std min_data=',min_data_std)\n",
    "print('<durada_consum>=',durada_consum_mean,'', 'std durada_consum=',durada_consum_std)\n",
    "print('<durada>=',durada_mean,'', 'std durada=',durada_std)\n",
    "\n",
    "\n",
    "# normalize / standarize\n",
    "df[columns_to_normalize] = df[columns_to_normalize].apply(lambda x: (x - x.mean()) / np.std(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-intro",
   "metadata": {},
   "source": [
    "## 1.2- Ideal number of clústers (cost function)\n",
    "\n",
    "· We determine the ideal number of \"k\" clusters by applying the algorithm for k=1,2,...,10 clusters and representing the cost function, defined as the sum of all the distances of the samples to their respective centroids. Since one expects great distances for small k and small distances for large k, the point where the cost function stops decreasing \"abruptly\" (Elbow point), it is what we choose as \"ideal k\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_kideal=time.time()\n",
    "\n",
    "cost = []\n",
    "k = []\n",
    "for num_clusters in list(range(1,10)):\n",
    "    kproto = KPrototypes(n_clusters=num_clusters, init='Cao',n_jobs = -1)\n",
    "    kproto.fit_predict(df2, categorical=[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n",
    "    cost.append(kproto.cost_)\n",
    "    k.append(num_clusters)\n",
    "\n",
    "final_time_kideal=time.time()\n",
    "\n",
    "print('El temps que ha tardat és de:', (final_time_kideal-start_time_kideal)/3600,'','hores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,5))    # PLOT COST (TOTAL DISTANCE) VS K (NUMBER OF CLUSTERS)\n",
    "\n",
    "ax.plot(k, cost, '+-',)    \n",
    "ax.set_xlabel('number of clusters k',fontsize=16)\n",
    "ax.set_ylabel('cost',fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-psychiatry",
   "metadata": {},
   "source": [
    "## 1.3- Clustering. K-Prototypes Algorithm\n",
    "\n",
    "\n",
    "We apply the clustering algorithm to the DataFrame specifying the discrete/categorical columns. The algorithm returns:\n",
    "\n",
    "    1. The position of the \"centroids\", i.e. the centres of each cluster.\n",
    "    2. The cluster labels for each consumption item.\n",
    "    3. The cost.\n",
    "    4. The number of iterations used to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_clustering=time.time()\n",
    "\n",
    "df.to_numpy()\n",
    "kproto = KPrototypes(n_clusters=5, init='Cao',n_jobs = -1)  # we chose k=4\n",
    "clusters = kproto.fit_predict(df, categorical=[ 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,16,17,18])\n",
    "\n",
    "final_time_clustering=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El temps que ha tardat el k-prototypes a clusteritzar les dades ha estat de:', (final_time_clustering-start_time_clustering)/3600,'','hores')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('Les coordenades dels Centrodis són:')\n",
    "print('')\n",
    "print(kproto.cluster_centroids_)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('El cost és de:', kproto.cost_)\n",
    "print('')\n",
    "\n",
    "print('Nombre de iteracions:', kproto.n_iter_)\n",
    "print('')\n",
    "\n",
    "print('Clusters (labels)')\n",
    "print(kproto.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
